# ğŸ”„ Flujo Completo de EjecuciÃ³n del Chat - TenderAI

**Documento tÃ©cnico:** Explica paso a paso TODO lo que sucede cuando el usuario envÃ­a un mensaje en el chat, incluyendo quÃ© se envÃ­a al LLM en cada momento.

---

## ğŸ“‹ Ãndice

1. [VisiÃ³n General](#visiÃ³n-general)
2. [Flujo Paso a Paso](#flujo-paso-a-paso)
3. [Mensajes Enviados al LLM](#mensajes-enviados-al-llm)
4. [Ejemplos Reales](#ejemplos-reales)
5. [Diagrama de Flujo](#diagrama-de-flujo)

---

## VisiÃ³n General

```
Usuario escribe mensaje â†’ Django View â†’ Chat Service â†’ Agent Graph â†’ LLM (1-3 veces) â†’ Respuesta
                                                           â†“
                                                      ChromaDB (si necesita docs)
```

**Llamadas al LLM:**
- **MÃ­nimo:** 1 llamada (mensaje directo sin documentos)
- **Normal:** 2 llamadas (routing + respuesta)
- **Con grading:** 2-8 llamadas (routing + grading por documento + respuesta)

---

## Flujo Paso a Paso

### ğŸ¯ PASO 1: Usuario EnvÃ­a Mensaje

**Archivo:** `chat/views.py` â†’ `ChatMessageCreateView.post()`

**LÃ­neas:** 61-86

```python
# Usuario escribe: "cual es la licitacion con mas dinero"
user_message_content = request.POST.get('message', '').strip()

# Se crea el mensaje en BD
user_message = ChatMessage.objects.create(
    session=session,
    role='user',
    content=user_message_content  # "cual es la licitacion con mas dinero"
)
```

**Logs en consola:**
```
======================================================================
[CHAT REQUEST] Usuario: pepe2012 (OLLAMA)
[CHAT REQUEST] SesiÃ³n ID: 42 | TÃ­tulo: Consulta licitaciones
[CHAT REQUEST] Mensaje: cual es la licitacion con mas dinero
======================================================================
```

---

### ğŸ¯ PASO 2: Preparar Historial Conversacional

**Archivo:** `chat/views.py` â†’ lÃ­neas 98-120

```python
# Obtener mensajes anteriores de la sesiÃ³n
previous_messages = session.messages.filter(
    created_at__lt=user_message.created_at
).order_by('created_at')

# Convertir a formato para el agente
conversation_history = []
for msg in previous_messages:
    conversation_history.append({
        'role': msg.role,  # 'user' o 'assistant'
        'content': msg.content
    })
```

**Ejemplo de historial:**
```python
[
    {'role': 'user', 'content': 'hola'},
    {'role': 'assistant', 'content': 'Â¡Hola! Â¿En quÃ© puedo ayudarte?'},
    {'role': 'user', 'content': 'busca licitaciones de software'},
    {'role': 'assistant', 'content': 'He encontrado 6 licitaciones...'}
]
```

---

### ğŸ¯ PASO 3: Crear ChatAgentService

**Archivo:** `chat/services.py` â†’ `ChatAgentService.__init__()`

**LÃ­neas:** 28-75

```python
chat_service = ChatAgentService(request.user)

# Lee configuraciÃ³n del usuario
self.provider = user.llm_provider  # "ollama"
self.model = user.ollama_model  # "qwen2.5:7b"
self.embedding_model = user.ollama_embedding_model  # "nomic-embed-text"
self.api_key = user.llm_api_key  # None para Ollama
```

**Logs en consola:**
```
[SERVICE] Inicializando process_message...
[SERVICE] Proveedor: OLLAMA
[SERVICE] Modelo LLM: qwen2.5:7b
[SERVICE] Modelo Embeddings: nomic-embed-text:latest
```

---

### ğŸ¯ PASO 4: Crear Agente RAG

**Archivo:** `chat/services.py` â†’ `_get_agent()`

**LÃ­neas:** 82-160

```python
# Se crea el agente con:
agent = RAGAgent(
    llm_provider=self.provider,  # "ollama"
    llm_model=self.model,  # "qwen2.5:7b"
    vectorstore=vectorstore,  # ConexiÃ³n a ChromaDB
    use_grading=self.user.use_grading,  # True/False
    use_verification=self.user.use_verification  # True/False
)
```

**Componentes inicializados:**

1. **LLM (Ollama):**
```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="qwen2.5:7b",
    temperature=0.3,  # Desde .env
    num_ctx=2048,  # Contexto de tokens desde .env
    base_url="http://localhost:11434"
)
```

2. **Retriever (ChromaDB):**
```python
retriever = vectorstore.as_retriever(
    search_kwargs={'k': 6}  # Recuperar 6 documentos
)
```

**Logs en consola:**
```
[SERVICE] Creando agente RAG...
INFO:agent_ia_core.agent_graph:Inicializando LLM: ollama - qwen2.5:7b
INFO:agent_ia_core.agent_graph:Inicializando Ollama local con modelo: qwen2.5:7b
INFO:index_build:Cargando Ã­ndice existente desde data/index/chroma
INFO:index_build:âœ“ Ãndice cargado: 235 documentos
[SERVICE] âœ“ Agente creado correctamente
```

---

### ğŸ¯ PASO 5: Ejecutar Query en el Agente

**Archivo:** `chat/services.py` â†’ `process_message()`

**LÃ­neas:** 252-284

```python
# Pasar mensaje PURO (sin historial concatenado) + historial separado
result = agent.query(
    question=enriched_message,  # "cual es la licitacion con mas dinero"
    conversation_history=formatted_history  # [{role, content}, ...]
)
```

**El agente recibe:**
```python
{
    "question": "cual es la licitacion con mas dinero",
    "conversation_history": [
        {'role': 'user', 'content': 'hola'},
        {'role': 'assistant', 'content': 'Â¡Hola! Â¿En quÃ© puedo ayudarte?'}
    ]
}
```

**Logs en consola:**
```
[SERVICE] Ejecutando query en el agente...
[SERVICE] Mensaje puro (para routing): cual es la licitacion con mas dinero...
[SERVICE] Historial: 2 mensajes
```

---

### ğŸ¯ PASO 6: Iniciar Graph State

**Archivo:** `agent_ia_core/agent_graph.py` â†’ `query()`

**LÃ­neas:** 529-558

```python
initial_state = {
    "question": "cual es la licitacion con mas dinero",  # Solo pregunta actual
    "messages": [],  # VacÃ­o por ahora
    "documents": [],  # Se llenarÃ¡ si busca docs
    "relevant_documents": [],  # Docs despuÃ©s de grading
    "answer": "",  # Se llenarÃ¡ al final
    "route": "",  # "vectorstore" o "general"
    "verified_fields": [],  # Para verificaciÃ³n XML
    "iteration": 0,
    "conversation_history": [  # Historial separado
        {'role': 'user', 'content': 'hola'},
        {'role': 'assistant', 'content': 'Â¡Hola! Â¿En quÃ© puedo ayudarte?'}
    ]
}
```

**Logs en consola:**
```
INFO:agent_ia_core.agent_graph:
============================================================
INFO:agent_ia_core.agent_graph:CONSULTA: cual es la licitacion con mas dinero
INFO:agent_ia_core.agent_graph:HISTORIAL: 2 mensajes previos
INFO:agent_ia_core.agent_graph:
============================================================
```

---

## ğŸ¤– LLAMADAS AL LLM

### ğŸ“ LLAMADA #1: ROUTING (ClasificaciÃ³n)

**Archivo:** `agent_ia_core/agent_graph.py` â†’ `_route_node()`

**LÃ­neas:** 262-320

**PropÃ³sito:** Decidir si necesita buscar documentos o es conversaciÃ³n general.

#### Prompt enviado al LLM:

**SYSTEM MESSAGE:**
```
Eres un clasificador de consultas para un sistema de licitaciones pÃºblicas.

Tu trabajo es decidir si el usuario necesita buscar en la base de datos de licitaciones.

**IMPORTANTE: Analiza el CONTEXTO COMPLETO de la conversaciÃ³n, no solo el mensaje aislado.**

CategorÃ­as:
1) "vectorstore" - El usuario pregunta por licitaciones/ofertas/contratos ESPECÃFICOS que estÃ¡n en la base de datos
   Ejemplos:
   - "cual es la mejor licitaciÃ³n en software"
   - "busca ofertas para desarrollo web"
   - "muÃ©strame contratos disponibles"
   - "quÃ© licitaciones hay en construcciÃ³n"
   - "propuestas interesantes para mi empresa"

   **CLAVE:** Si la conversaciÃ³n ya estÃ¡ hablando de licitaciones especÃ­ficas, preguntas como
   "cuÃ¡nto dinero podrÃ­a ganar", "cuÃ¡l es el presupuesto", "cuÃ¡ndo es el plazo" tambiÃ©n necesitan vectorstore.

2) "general" - ConversaciÃ³n general, saludos, o preguntas conceptuales que NO requieren buscar en documentos
   Ejemplos:
   - "hola, quÃ© tal"
   - "quÃ© es una licitaciÃ³n pÃºblica" (concepto general)
   - "cÃ³mo funciona el proceso de licitaciÃ³n" (explicaciÃ³n)
   - "gracias por la ayuda"

REGLAS CRÃTICAS:
- Si el usuario pregunta por licitaciones/ofertas/contratos CONCRETOS que podrÃ­an estar en la base de datos â†’ vectorstore
- Si la conversaciÃ³n YA ESTÃ hablando de licitaciones especÃ­ficas y el usuario hace preguntas de seguimiento â†’ vectorstore
- Si es pregunta conceptual, saludo, o explicaciÃ³n sin contexto de licitaciones especÃ­ficas â†’ general

Responde SOLO con la categorÃ­a: "vectorstore" o "general" (sin explicaciones).
```

**HUMAN MESSAGE:**
```
Contexto de la conversaciÃ³n:
Usuario: hola...
Asistente: Â¡Hola! Â¿En quÃ© puedo ayudarte?...

---

Mensaje actual del usuario:
"cual es la licitacion con mas dinero"

Considerando el CONTEXTO COMPLETO de la conversaciÃ³n, Â¿necesita buscar en la base de datos de licitaciones?
CategorÃ­a (vectorstore o general):
```

#### Respuesta del LLM:
```
vectorstore
```

**HTTP Request a Ollama:**
```http
POST http://localhost:11434/api/chat
{
  "model": "qwen2.5:7b",
  "messages": [
    {"role": "system", "content": "Eres un clasificador de consultas..."},
    {"role": "user", "content": "Contexto de la conversaciÃ³n:\nUsuario: hola...\n\n---\n\nMensaje actual del usuario:\n\"cual es la licitacion con mas dinero\"\n\nCategorÃ­a (vectorstore o general):"}
  ],
  "temperature": 0.3,
  "stream": false
}
```

**Logs en consola:**
```
[ROUTE] Clasificando mensaje CON contexto: cual es la licitacion con mas dinero
[ROUTE] Historial disponible: 2 mensajes
INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
[ROUTE] LLM clasificÃ³ como DOCUMENTOS (respuesta: vectorstore)
[ROUTE] Ruta final decidida: vectorstore
```

**Resultado:** `state["route"] = "vectorstore"`

---

### ğŸ“ LLAMADA #2: RETRIEVE (BÃºsqueda de Documentos)

**Archivo:** `agent_ia_core/agent_graph.py` â†’ `_retrieve_node()`

**LÃ­neas:** 322-365

**PropÃ³sito:** Buscar documentos relevantes en ChromaDB.

#### Proceso:

1. **Generar embedding del mensaje:**
```python
# Se convierte el mensaje a vector con Ollama embeddings
query_embedding = embeddings.embed_query("cual es la licitacion con mas dinero")
# Resultado: [0.123, -0.456, 0.789, ...] (vector de 768 dimensiones)
```

**HTTP Request a Ollama:**
```http
POST http://localhost:11434/api/embed
{
  "model": "nomic-embed-text",
  "input": "cual es la licitacion con mas dinero"
}
```

2. **Buscar en ChromaDB:**
```python
# ChromaDB busca los 6 documentos mÃ¡s similares
results = vectorstore.similarity_search(
    query="cual es la licitacion con mas dinero",
    k=6
)
```

#### Documentos recuperados (ejemplo):

```python
[
    Document(
        page_content="Presupuesto estimado: 961,200.00 EUR",
        metadata={
            'ojs_notice_id': '00668461-2025',
            'section': 'budget',
            'buyer_name': 'FundaciÃ³n Estatal',
            'cpv_codes': '72267100',
            'budget_eur': '961200.0',  # â† AHORA DISPONIBLE
            'publication_date': '2025-09-15'
        }
    ),
    Document(
        page_content="Presupuesto estimado: 750,000.00 EUR",
        metadata={
            'ojs_notice_id': '00677736-2025',
            'section': 'budget',
            'buyer_name': 'Autoridad Portuaria',
            'cpv_codes': '72267100,72212000',
            'budget_eur': '750000.0',
            'publication_date': '2025-09-20'
        }
    ),
    # ... 4 documentos mÃ¡s
]
```

**Logs en consola:**
```
[RETRIEVE] Buscando documentos para: cual es la licitacion con mas dinero
INFO:retriever:Recuperando documentos: query='cual es la licitacion con mas dinero', k=6, filters={}
INFO:httpx:HTTP Request: POST http://localhost:11434/api/embed "HTTP/1.1 200 OK"
INFO:retriever:Recuperados 6 documentos (de 18 candidatos)
[RETRIEVE] Recuperados 6 documentos
```

**Resultado:** `state["documents"] = [doc1, doc2, doc3, doc4, doc5, doc6]`

---

### ğŸ“ LLAMADAS #3-8: GRADING (Opcional - si use_grading=True)

**Archivo:** `agent_ia_core/agent_graph.py` â†’ `_grade_node()`

**LÃ­neas:** 367-407

**PropÃ³sito:** Evaluar si cada documento es realmente relevante.

**Se hace UNA llamada al LLM POR CADA DOCUMENTO (6 llamadas en este caso).**

#### Prompt por documento:

**SYSTEM MESSAGE:**
```
Eres un evaluador de relevancia de documentos.

Tu tarea es determinar si un documento recuperado es relevante para responder la pregunta del usuario.

Criterios de relevancia:
- El documento contiene informaciÃ³n directamente relacionada con la pregunta
- El documento puede ayudar a responder total o parcialmente la pregunta
- El contenido es especÃ­fico y no genÃ©rico

Si NO es relevante, identifica internamente una razÃ³n breve (para logging).
Responde SOLO con "yes" o "no".
```

**HUMAN MESSAGE (ejemplo para documento 1):**
```
Pregunta: cual es la licitacion con mas dinero

Documento:
ID: 00668461-2025
SecciÃ³n: budget
Contenido: Presupuesto estimado: 961,200.00 EUR

Â¿Es este documento relevante para responder la pregunta?
Responde solo "yes" o "no":
```

#### Respuesta del LLM:
```
yes
```

**HTTP Requests (6 llamadas):**
```http
POST http://localhost:11434/api/chat  # Doc 1 â†’ yes
POST http://localhost:11434/api/chat  # Doc 2 â†’ yes
POST http://localhost:11434/api/chat  # Doc 3 â†’ yes
POST http://localhost:11434/api/chat  # Doc 4 â†’ no
POST http://localhost:11434/api/chat  # Doc 5 â†’ yes
POST http://localhost:11434/api/chat  # Doc 6 â†’ yes
```

**Logs en consola:**
```
[GRADE] Evaluando relevancia de 6 documentos
[GRADE] Doc 1/6: yes - Presupuesto estimado: 961,200.00 EUR
[GRADE] Doc 2/6: yes - Presupuesto estimado: 750,000.00 EUR
[GRADE] Doc 3/6: yes - Presupuesto estimado: 500,000.00 EUR
[GRADE] Doc 4/6: no - Requisitos de elegibilidad segÃºn...
[GRADE] Doc 5/6: yes - Presupuesto estimado: 373,831.76 EUR
[GRADE] Doc 6/6: yes - Presupuesto estimado: 300,000.00 EUR
[GRADE] Documentos relevantes: 5/6
```

**Resultado:** `state["relevant_documents"] = [doc1, doc2, doc3, doc5, doc6]` (5 docs)

---

### ğŸ“ LLAMADA #9: ANSWER (GeneraciÃ³n de Respuesta)

**Archivo:** `agent_ia_core/agent_graph.py` â†’ `_answer_node()`

**LÃ­neas:** 409-491

**PropÃ³sito:** Generar la respuesta final usando documentos + historial.

#### ConstrucciÃ³n del prompt:

**Paso 1: Construir contexto con historial**
```python
def build_context_with_history(current_question: str) -> str:
    if not conversation_history:
        return current_question

    history_text = "Historial de la conversaciÃ³n:\n"
    for msg in conversation_history:
        role_label = "Usuario" if msg['role'] == 'user' else "Asistente"
        history_text += f"{role_label}: {msg['content']}\n"

    return f"{history_text}\n---\n\nPregunta actual del usuario:\n{current_question}"
```

**Resultado:**
```
Historial de la conversaciÃ³n:
Usuario: hola
Asistente: Â¡Hola! Â¿En quÃ© puedo ayudarte?

---

Pregunta actual del usuario:
cual es la licitacion con mas dinero
```

**Paso 2: Formatear documentos**

Usando `create_answer_prompt()` de `agent_ia_core/prompts.py`:

```python
context_text = """
[Documento 1]
ID: 00668461-2025
SecciÃ³n: budget
Comprador: FundaciÃ³n Estatal
CPV: 72267100
Presupuesto: 961200.0 EUR
Publicado: 2025-09-15
Contenido:
Presupuesto estimado: 961,200.00 EUR

---

[Documento 2]
ID: 00677736-2025
SecciÃ³n: budget
Comprador: Autoridad Portuaria
CPV: 72267100,72212000
Presupuesto: 750000.0 EUR
Publicado: 2025-09-20
Contenido:
Presupuesto estimado: 750,000.00 EUR

---

[Documento 3]
ID: 00670256-2025
SecciÃ³n: budget
Comprador: Ajuntament de ValÃ¨ncia
CPV: 72267100,48000000
Presupuesto: 500000.0 EUR
Publicado: 2025-09-18
Contenido:
Presupuesto estimado: 500,000.00 EUR

---

[Documento 4]
ID: 00623257-2025
SecciÃ³n: budget
Comprador: Consejo Insular de Aguas
CPV: 79341000,79341400
Presupuesto: 373831.76 EUR
Publicado: 2025-09-24
Contenido:
Presupuesto estimado: 373,831.76 EUR

---

[Documento 5]
ID: 00660806-2025
SecciÃ³n: budget
Comprador: Ayuntamiento de El Sauzal
CPV: 72000000,48900000
Presupuesto: 300000.0 EUR
Publicado: 2025-09-22
Contenido:
Presupuesto estimado: 300,000.00 EUR
"""
```

#### Prompt completo enviado al LLM:

**SYSTEM MESSAGE:**
```
Eres un asistente conversacional natural y Ãºtil. Tu especialidad es ayudar con licitaciones pÃºblicas, pero puedes hablar de cualquier tema.

**CÃ³mo eres:**
- Conversas de forma natural, como un humano amigable
- Respondes de manera clara y directa
- Te adaptas al tono del usuario (formal/informal)
- Eres Ãºtil y prÃ¡ctico

**Tu conocimiento:**
- Conoces sobre licitaciones pÃºblicas, TED (Tenders Electronic Daily), CPV, plazos, presupuestos
- Tienes acceso a documentos oficiales cuando hay consultas especÃ­ficas

**Lo importante:**
- Cuando tengas documentos, Ãºsalos para dar informaciÃ³n precisa
- Cuando NO tengas documentos, responde natural basÃ¡ndote en tu conocimiento general
- Si algo no lo sabes o no estÃ¡ en los documentos, dilo honestamente
- Puedes usar Markdown para formatear (listas, **negritas**, tablas, etc.)

Responde de la forma mÃ¡s natural y Ãºtil posible. No te limites a fÃ³rmulas rÃ­gidas.
```

**HUMAN MESSAGE:**
```
Historial de la conversaciÃ³n:
Usuario: hola
Asistente: Â¡Hola! Â¿En quÃ© puedo ayudarte?

---

Pregunta actual del usuario:
cual es la licitacion con mas dinero

---

Tienes acceso a estos documentos de licitaciones:

[Documento 1]
ID: 00668461-2025
SecciÃ³n: budget
Comprador: FundaciÃ³n Estatal
CPV: 72267100
Presupuesto: 961200.0 EUR
Publicado: 2025-09-15
Contenido:
Presupuesto estimado: 961,200.00 EUR

---

[Documento 2]
ID: 00677736-2025
SecciÃ³n: budget
Comprador: Autoridad Portuaria
CPV: 72267100,72212000
Presupuesto: 750000.0 EUR
Publicado: 2025-09-20
Contenido:
Presupuesto estimado: 750,000.00 EUR

---

[Documento 3]
ID: 00670256-2025
SecciÃ³n: budget
Comprador: Ajuntament de ValÃ¨ncia
CPV: 72267100,48000000
Presupuesto: 500000.0 EUR
Publicado: 2025-09-18
Contenido:
Presupuesto estimado: 500,000.00 EUR

---

[Documento 4]
ID: 00623257-2025
SecciÃ³n: budget
Comprador: Consejo Insular de Aguas
CPV: 79341000,79341400
Presupuesto: 373831.76 EUR
Publicado: 2025-09-24
Contenido:
Presupuesto estimado: 373,831.76 EUR

---

[Documento 5]
ID: 00660806-2025
SecciÃ³n: budget
Comprador: Ayuntamiento de El Sauzal
CPV: 72000000,48900000
Presupuesto: 300000.0 EUR
Publicado: 2025-09-22
Contenido:
Presupuesto estimado: 300,000.00 EUR

---

El usuario pregunta: cual es la licitacion con mas dinero

Usa la informaciÃ³n de los documentos para responder. SÃ© Ãºtil y claro. Si usas datos especÃ­ficos de los documentos, cita la fuente con [ID | secciÃ³n].

Respuesta:
```

#### Respuesta del LLM:

```
BasÃ¡ndome en los documentos proporcionados, la licitaciÃ³n con el presupuesto mÃ¡s alto es:

**LicitaciÃ³n 00668461-2025** [00668461-2025 | budget]
- **Comprador:** FundaciÃ³n Estatal
- **Presupuesto:** 961,200.00 EUR
- **CPV:** 72267100
- **Fecha de publicaciÃ³n:** 2025-09-15

Esta es la licitaciÃ³n mÃ¡s grande en tÃ©rminos de presupuesto entre las disponibles.

Si necesitas mÃ¡s detalles sobre esta licitaciÃ³n o quieres explorar otras opciones, no dudes en preguntar.
```

**HTTP Request:**
```http
POST http://localhost:11434/api/chat
{
  "model": "qwen2.5:7b",
  "messages": [
    {
      "role": "system",
      "content": "Eres un asistente conversacional natural y Ãºtil..."
    },
    {
      "role": "user",
      "content": "Historial de la conversaciÃ³n:\nUsuario: hola\nAsistente: Â¡Hola! Â¿En quÃ© puedo ayudarte?\n\n---\n\nPregunta actual del usuario:\ncual es la licitacion con mas dinero\n\n---\n\nTienes acceso a estos documentos de licitaciones:\n\n[Documento 1]\nID: 00668461-2025\n..."
    }
  ],
  "temperature": 0.3,
  "num_ctx": 2048,
  "stream": false
}
```

**Logs en consola:**
```
[ANSWER] Generando respuesta
[ANSWER] Usando historial de 2 mensajes para contexto
[ANSWER] Respuesta con 5 documentos
INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
[ANSWER] Respuesta generada (285 caracteres)
```

**Resultado:** `state["answer"] = "BasÃ¡ndome en los documentos proporcionados..."`

---

### ğŸ¯ PASO 7: Guardar Respuesta en BD

**Archivo:** `chat/views.py` â†’ lÃ­neas 130-160

```python
# Crear mensaje del asistente
assistant_message = ChatMessage.objects.create(
    session=session,
    role='assistant',
    content=response['content'],  # La respuesta del LLM
    metadata={
        'route': response['metadata'].get('route'),  # "vectorstore"
        'num_documents': response['metadata'].get('num_documents'),  # 5
        'total_tokens': response['metadata'].get('total_tokens'),  # 450
        'cost_eur': response['metadata'].get('cost_eur')  # 0.0000 (Ollama)
    }
)
```

**Logs en consola:**
```
[SERVICE] âœ“ Query ejecutado correctamente
[SERVICE] âœ“ Respuesta procesada: 285 caracteres
[SERVICE] Documentos recuperados: 5
[SERVICE] Tokens totales: 450 (in: 220, out: 230)
[SERVICE] Costo: â‚¬0.0000
```

---

### ğŸ¯ PASO 8: Enviar Respuesta al Frontend

**Archivo:** `chat/views.py` â†’ lÃ­neas 180-195

```python
return JsonResponse({
    'success': True,
    'message': {
        'id': assistant_message.id,
        'content': assistant_message.content,
        'created_at': assistant_message.created_at.isoformat(),
        'role': 'assistant',
        'metadata': assistant_message.metadata
    }
})
```

**JSON enviado al navegador:**
```json
{
  "success": true,
  "message": {
    "id": 1234,
    "content": "BasÃ¡ndome en los documentos proporcionados, la licitaciÃ³n con el presupuesto mÃ¡s alto es:\n\n**LicitaciÃ³n 00668461-2025**...",
    "created_at": "2025-01-19T14:30:45.123456",
    "role": "assistant",
    "metadata": {
      "route": "vectorstore",
      "num_documents": 5,
      "total_tokens": 450,
      "cost_eur": 0.0
    }
  }
}
```

---

## ğŸ“Š Resumen de Llamadas al LLM

Para la query: **"cual es la licitacion con mas dinero"** (con `use_grading=True`)

| # | Tipo | PropÃ³sito | Input Tokens | Output Tokens | Modelo |
|---|------|-----------|--------------|---------------|--------|
| 1 | Routing | Clasificar query | ~50 | ~5 | qwen2.5:7b |
| 2 | Embed | Generar vector de bÃºsqueda | ~15 | 768 dim | nomic-embed-text |
| 3 | Grading | Evaluar doc 1 | ~30 | ~3 | qwen2.5:7b |
| 4 | Grading | Evaluar doc 2 | ~30 | ~3 | qwen2.5:7b |
| 5 | Grading | Evaluar doc 3 | ~30 | ~3 | qwen2.5:7b |
| 6 | Grading | Evaluar doc 4 | ~30 | ~3 | qwen2.5:7b |
| 7 | Grading | Evaluar doc 5 | ~30 | ~3 | qwen2.5:7b |
| 8 | Grading | Evaluar doc 6 | ~30 | ~3 | qwen2.5:7b |
| 9 | Answer | Generar respuesta | ~220 | ~230 | qwen2.5:7b |
| **TOTAL** | | | **~465** | **~256** | |

**Con `use_grading=False`:** Solo 3 llamadas (routing, embed, answer) = ~285 tokens total

---

## ğŸ”„ Diagrama de Flujo Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USUARIO ENVÃA MENSAJE                        â”‚
â”‚                "cual es la licitacion con mas dinero"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 1: Django View (chat/views.py)                            â”‚
â”‚  - Crear ChatMessage en BD                                       â”‚
â”‚  - Obtener historial de conversaciÃ³n                            â”‚
â”‚  - Logs: [CHAT REQUEST] Usuario, SesiÃ³n, Mensaje                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 2: ChatAgentService (chat/services.py)                    â”‚
â”‚  - Leer config del usuario (provider, modelo, API key)          â”‚
â”‚  - Crear agente RAG con LLM + Retriever                         â”‚
â”‚  - Logs: [SERVICE] Proveedor, Modelo LLM, Modelo Embeddings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 3: Iniciar Agent Graph (agent_ia_core/agent_graph.py)     â”‚
â”‚  - Crear initial_state con question + conversation_history      â”‚
â”‚  - Logs: [CONSULTA] Pregunta, [HISTORIAL] X mensajes            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– LLAMADA LLM #1: ROUTING                                      â”‚
â”‚                                                                  â”‚
â”‚  Input:                                                          â”‚
â”‚  - System: "Eres un clasificador de consultas..."               â”‚
â”‚  - Human: "Contexto:\nUsuario: hola\n...\n---\nMensaje actual:  â”‚
â”‚            cual es la licitacion con mas dinero"                â”‚
â”‚                                                                  â”‚
â”‚  Output: "vectorstore"                                           â”‚
â”‚                                                                  â”‚
â”‚  HTTP: POST localhost:11434/api/chat                             â”‚
â”‚  Logs: [ROUTE] ClasificÃ³ como DOCUMENTOS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  route=vectorstore â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ” LLAMADA #2: EMBED (Generar vector)                           â”‚
â”‚                                                                  â”‚
â”‚  Input: "cual es la licitacion con mas dinero"                  â”‚
â”‚  Output: [0.123, -0.456, 0.789, ...] (768 dimensiones)          â”‚
â”‚                                                                  â”‚
â”‚  HTTP: POST localhost:11434/api/embed                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“š BUSCAR EN CHROMADB                                           â”‚
â”‚  - Similarity search con vector                                 â”‚
â”‚  - Recuperar top 6 documentos                                   â”‚
â”‚  - Logs: [RETRIEVE] Recuperados 6 documentos                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   use_grading=True?             â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ YES                 â”‚ NO
                 â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  ğŸ¤– LLAMADAS #3-8: GRADING â”‚         â”‚
â”‚                            â”‚         â”‚
â”‚  Por cada documento (6):   â”‚         â”‚
â”‚  - System: "Evaluador..."  â”‚         â”‚
â”‚  - Human: "Pregunta: ...   â”‚         â”‚
â”‚           Documento: ...   â”‚         â”‚
â”‚           Â¿Relevante?"     â”‚         â”‚
â”‚  - Output: "yes" o "no"    â”‚         â”‚
â”‚                            â”‚         â”‚
â”‚  HTTP: 6x POST /api/chat   â”‚         â”‚
â”‚  Logs: [GRADE] 5/6 docs OK â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
             â”‚                         â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– LLAMADA #9: ANSWER (Generar respuesta)                       â”‚
â”‚                                                                  â”‚
â”‚  Input:                                                          â”‚
â”‚  - System: "Eres un asistente conversacional natural..."        â”‚
â”‚  - Human: "Historial:\nUsuario: hola\nAsistente: ...\n---\n     â”‚
â”‚            Pregunta actual: cual es...\n---\n                   â”‚
â”‚            Documentos:\n[Documento 1]\nID: 00668461-2025\n      â”‚
â”‚            Presupuesto: 961200.0 EUR\n...\n[Documento 2]..."    â”‚
â”‚                                                                  â”‚
â”‚  Output: "BasÃ¡ndome en los documentos proporcionados, la        â”‚
â”‚           licitaciÃ³n con el presupuesto mÃ¡s alto es:\n          â”‚
â”‚           **LicitaciÃ³n 00668461-2025**..."                      â”‚
â”‚                                                                  â”‚
â”‚  HTTP: POST localhost:11434/api/chat                             â”‚
â”‚  Logs: [ANSWER] Respuesta generada (285 caracteres)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 4: Guardar en BD (chat/views.py)                          â”‚
â”‚  - Crear ChatMessage con role='assistant'                       â”‚
â”‚  - Guardar metadata (route, num_documents, tokens, cost)        â”‚
â”‚  - Logs: [SERVICE] âœ“ Respuesta procesada                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 5: Enviar JSON al Frontend                                â”‚
â”‚  {                                                               â”‚
â”‚    "success": true,                                              â”‚
â”‚    "message": {                                                  â”‚
â”‚      "id": 1234,                                                 â”‚
â”‚      "content": "BasÃ¡ndome en los documentos...",               â”‚
â”‚      "role": "assistant",                                        â”‚
â”‚      "metadata": {                                               â”‚
â”‚        "route": "vectorstore",                                   â”‚
â”‚        "num_documents": 5,                                       â”‚
â”‚        "total_tokens": 450                                       â”‚
â”‚      }                                                            â”‚
â”‚    }                                                              â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USUARIO VE RESPUESTA                         â”‚
â”‚  "BasÃ¡ndome en los documentos proporcionados, la licitaciÃ³n con â”‚
â”‚   el presupuesto mÃ¡s alto es: **LicitaciÃ³n 00668461-2025**..."  â”‚
â”‚                                                                  â”‚
â”‚  5 documento(s) consultado(s)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Ejemplos Reales

### Ejemplo 1: Mensaje Simple (Sin Documentos)

**Input:** "hola"

**Llamadas al LLM:**
1. **Routing:** "general"
2. **Answer:** Respuesta directa sin docs

**Total:** 2 llamadas, ~80 tokens

---

### Ejemplo 2: Pregunta EspecÃ­fica (Con Documentos)

**Input:** "cual es la licitacion con mas dinero"

**Llamadas al LLM:**
1. **Routing:** "vectorstore"
2. **Embed:** Generar vector
3. **Grading x6:** Evaluar cada doc
4. **Answer:** Generar respuesta con docs

**Total:** 9 llamadas, ~465 tokens

---

### Ejemplo 3: Pregunta de Seguimiento (Contextual)

**Historial:**
- Usuario: "busca licitaciones de software"
- Asistente: "He encontrado 6 licitaciones..."

**Input:** "cuanto dinero podria ganar"

**Llamadas al LLM:**
1. **Routing CON CONTEXTO:** "vectorstore" (detecta continuaciÃ³n)
2. **Embed:** Generar vector
3. **Grading x6:** Evaluar cada doc
4. **Answer CON HISTORIAL:** Generar respuesta con contexto

**Total:** 9 llamadas, ~520 tokens (mÃ¡s largo por historial)

---

## âš™ï¸ ConfiguraciÃ³n (Variables de .env)

```bash
# LLM Settings
LLM_PROVIDER=ollama
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
LLM_TEMPERATURE=0.3
OLLAMA_CONTEXT_LENGTH=2048
LLM_TIMEOUT=120

# Retrieval Settings
DEFAULT_K_RETRIEVE=6
MIN_SIMILARITY_SCORE=0.5

# Agent Features
USE_GRADING=True
USE_XML_VERIFICATION=False

# Conversation
MAX_CONVERSATION_HISTORY=10
```

---

## ğŸ” Debugging

Para ver TODO el flujo en tiempo real, revisa los logs en la consola del servidor:

```bash
# Terminal donde corre: python manage.py runserver 8001

======================================================================
[CHAT REQUEST] Usuario: pepe2012 (OLLAMA)
[CHAT REQUEST] SesiÃ³n ID: 42 | TÃ­tulo: Consulta licitaciones
[CHAT REQUEST] Mensaje: cual es la licitacion con mas dinero
======================================================================
[SERVICE] Inicializando process_message...
[SERVICE] Proveedor: OLLAMA
[SERVICE] Modelo LLM: qwen2.5:7b
[SERVICE] Creando agente RAG...
INFO:agent_ia_core.agent_graph:CONSULTA: cual es la licitacion con mas dinero
[ROUTE] Clasificando mensaje CON contexto: cual es la licitacion con mas dinero
INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
[ROUTE] LLM clasificÃ³ como DOCUMENTOS (respuesta: vectorstore)
[RETRIEVE] Buscando documentos para: cual es la licitacion con mas dinero
INFO:httpx:HTTP Request: POST http://localhost:11434/api/embed "HTTP/1.1 200 OK"
INFO:retriever:Recuperados 6 documentos
[GRADE] Evaluando relevancia de 6 documentos
[GRADE] Documentos relevantes: 5/6
[ANSWER] Generando respuesta
INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
[ANSWER] Respuesta generada (285 caracteres)
[SERVICE] âœ“ Respuesta procesada: 285 caracteres
[SERVICE] Documentos recuperados: 5
[SERVICE] Tokens totales: 450
```

---

**Fecha de creaciÃ³n:** 2025-01-19
**VersiÃ³n del sistema:** v1.4.0
**Autor:** Claude Code Assistant
