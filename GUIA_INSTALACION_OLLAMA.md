# ü¶ô Gu√≠a de Instalaci√≥n de Ollama para TenderAI Platform

Esta gu√≠a te ense√±ar√° a instalar y configurar **Ollama** para ejecutar modelos de IA localmente en TenderAI Platform, con **m√°xima calidad**, **privacidad total** y **costo cero**.

---

## üìã Requisitos del Sistema

### Hardware Recomendado

Para ejecutar **Qwen2.5 72B** (el modelo recomendado para an√°lisis de licitaciones):

- **RAM**: 32GB+ (tu m√°quina con 32GB es perfecta)
- **GPU**: NVIDIA RTX 5080 (16GB VRAM) ‚úÖ EXCELENTE
- **Disco**: 50GB libres (para modelo + datos)
- **CPU**: Cualquier procesador moderno

### Hardware M√≠nimo

Si quieres probar con modelos m√°s peque√±os:

- **RAM**: 16GB
- **GPU**: NVIDIA GTX 1060 6GB o superior
- **Disco**: 10GB libres

---

## üöÄ Paso 1: Instalar Ollama

### Windows

1. **Descargar el instalador**:
   - Ve a: https://ollama.com/download/windows
   - Descarga `OllamaSetup.exe`

2. **Ejecutar el instalador**:
   - Doble clic en `OllamaSetup.exe`
   - Sigue las instrucciones del asistente
   - Se instalar√° en `C:\Program Files\Ollama\`

3. **Verificar instalaci√≥n**:
   ```cmd
   ollama --version
   ```
   Deber√≠as ver algo como: `ollama version 0.3.0`

### Linux / WSL

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS

```bash
brew install ollama
```

---

## ü§ñ Paso 2: Descargar el Modelo Qwen2.5 72B

Este es el **mejor modelo** para an√°lisis de licitaciones p√∫blicas debido a su:
- ‚úÖ Calidad comparable a GPT-4
- ‚úÖ Excelente razonamiento anal√≠tico
- ‚úÖ Mejor comprensi√≥n del espa√±ol t√©cnico
- ‚úÖ Capacidad de an√°lisis comparativo

### Descargar Qwen2.5 72B

```cmd
ollama pull qwen2.5:72b
```

**Tiempo estimado**: 15-30 minutos (descarga ~41GB)

### Verificar descarga

```cmd
ollama list
```

Deber√≠as ver:
```
NAME                MODIFIED      SIZE
qwen2.5:72b         2 hours ago   41GB
```

---

## üì¶ Paso 3: Descargar Modelo de Embeddings

Para vectorizaci√≥n de licitaciones, descarga **nomic-embed-text**:

```cmd
ollama pull nomic-embed-text
```

**Tiempo estimado**: 1-2 minutos (descarga ~274MB)

### Verificar embeddings

```cmd
ollama list
```

Deber√≠as ver:
```
NAME                    MODIFIED      SIZE
qwen2.5:72b             2 hours ago   41GB
nomic-embed-text        1 hour ago    274MB
```

---

## ‚öôÔ∏è Paso 4: Iniciar el Servidor Ollama

Ollama necesita estar corriendo en segundo plano.

### Windows

Ollama se inicia autom√°ticamente al instalarse. Verifica que est√© corriendo:

```cmd
ollama serve
```

Si ves `Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address`, significa que **ya est√° corriendo** ‚úÖ

### Linux / macOS

```bash
ollama serve
```

Deja esta terminal abierta.

---

## üîß Paso 5: Verificar que Funciona

### Probar el modelo de chat

```cmd
ollama run qwen2.5:72b
```

Escribe una pregunta de prueba:
```
>>> Analiza las ventajas de usar LLMs locales vs cloud para an√°lisis de licitaciones
```

Deber√≠as recibir una respuesta detallada en espa√±ol.

Para salir: escribe `/bye`

### Probar embeddings

```cmd
ollama run nomic-embed-text "Este es un texto de prueba para vectorizaci√≥n"
```

Deber√≠as ver un vector de n√∫meros (embedding generado).

---

## üéØ Paso 6: Configurar TenderAI Platform

### 6.1. Instalar langchain-ollama

```cmd
cd "c:\Users\andri\Desktop\Proyectos\Pagina web Agent_IA\Pagina web Agent_IA\TenderAI_Platform"
pip install langchain-ollama
```

### 6.2. Aplicar Migraciones de Base de Datos

```cmd
python manage.py makemigrations
python manage.py migrate
```

Esto a√±adir√° los campos `ollama_model` y `ollama_embedding_model` al modelo User.

### 6.3. Reiniciar el Servidor Django

```cmd
python manage.py runserver 8001
```

---

## üåê Paso 7: Configurar tu Perfil de Usuario

1. **Abrir tu navegador**:
   - Ve a: http://127.0.0.1:8001/perfil/

2. **Seleccionar Proveedor**:
   - En "Proveedor de IA", selecciona: **Ollama (Local)**

3. **Configurar Modelos**:
   - **Modelo Ollama**: `qwen2.5:72b`
   - **Modelo de Embeddings Ollama**: `nomic-embed-text`

4. **API Key**:
   - D√©jalo vac√≠o (Ollama no requiere API key)

5. **Guardar Cambios**

---

## ‚úÖ Paso 8: Probar la Integraci√≥n

### Opci√≥n A: Probar el Chat

1. Ve a: http://127.0.0.1:8001/chat/
2. Crea una nueva sesi√≥n de chat
3. Pregunta algo como:
   ```
   ¬øCu√°les son las licitaciones m√°s relevantes para mi empresa?
   ```

Deber√≠as ver:
- ‚úÖ Indicador "Pensando..." rotando
- ‚úÖ Respuesta generada por Qwen2.5 72B
- ‚úÖ Costo: **‚Ç¨0.00 (Gratis)** ‚Üê Sin cargos

### Opci√≥n B: Indexar Licitaciones

1. Ve a: http://127.0.0.1:8001/tenders/
2. Descarga algunas licitaciones XML
3. Ve a la pesta√±a "Vectorizaci√≥n"
4. Haz clic en "Indexar Todo"

Ver√°s:
- ‚úÖ Indexaci√≥n con embeddings de `nomic-embed-text`
- ‚úÖ Costo total: **‚Ç¨0.00 (Gratis)**
- ‚úÖ Sin l√≠mites de uso

---

## üé® Modelos Alternativos

Si Qwen2.5 72B es demasiado lento o necesitas probar otros modelos:

### Para CHAT

| Modelo | Tama√±o | Velocidad | Calidad | Comando |
|--------|--------|-----------|---------|---------|
| **Qwen2.5 72B** ‚≠ê | 41GB | Media | M√°xima | `ollama pull qwen2.5:72b` |
| Llama 3.3 70B | 40GB | Media | Muy Alta | `ollama pull llama3.3:70b` |
| DeepSeek-R1 14B | 9GB | R√°pida | Alta | `ollama pull deepseek-r1:14b` |
| Llama 3.1 8B | 4.7GB | Muy R√°pida | Media | `ollama pull llama3.1:8b` |
| Mistral 7B | 4.1GB | Muy R√°pida | Media-Alta | `ollama pull mistral:7b` |

### Para EMBEDDINGS

| Modelo | Tama√±o | Contexto | Calidad | Comando |
|--------|--------|----------|---------|---------|
| **nomic-embed-text** ‚≠ê | 274MB | 8192 tokens | Alta | `ollama pull nomic-embed-text` |
| mxbai-embed-large | 669MB | 512 tokens | Muy Alta (espa√±ol) | `ollama pull mxbai-embed-large` |

### Cambiar de Modelo

1. Descarga el nuevo modelo:
   ```cmd
   ollama pull llama3.3:70b
   ```

2. Ve a tu perfil: http://127.0.0.1:8001/perfil/
3. Cambia "Modelo Ollama" a: `llama3.3:70b`
4. Guarda cambios

---

## üîç Soluci√≥n de Problemas

### Problema 1: "Error: connect ECONNREFUSED 127.0.0.1:11434"

**Causa**: Ollama no est√° corriendo

**Soluci√≥n**:
```cmd
ollama serve
```

### Problema 2: "Error: model 'qwen2.5:72b' not found"

**Causa**: No has descargado el modelo

**Soluci√≥n**:
```cmd
ollama pull qwen2.5:72b
```

### Problema 3: Respuestas muy lentas

**Causa**: Modelo demasiado grande para tu GPU

**Soluciones**:
1. Usar un modelo m√°s peque√±o (ej: `deepseek-r1:14b`)
2. Cerrar otras aplicaciones para liberar VRAM
3. Verificar que la GPU est√© siendo utilizada:
   ```cmd
   nvidia-smi
   ```

### Problema 4: "Out of memory"

**Causa**: RAM o VRAM insuficiente

**Soluciones**:
1. Cerrar navegadores y aplicaciones pesadas
2. Usar un modelo m√°s peque√±o (ej: `mistral:7b`)
3. Reiniciar Ollama:
   ```cmd
   taskkill /F /IM ollama.exe
   ollama serve
   ```

### Problema 5: Modelo se descarga lentamente

**Causa**: Conexi√≥n lenta

**Soluci√≥n**:
- S√© paciente (modelos grandes tardan)
- Verifica velocidad de internet
- Pausa y resume la descarga:
  ```cmd
  # Si se interrumpe, simplemente vuelve a ejecutar:
  ollama pull qwen2.5:72b
  # Ollama resume desde donde se qued√≥
  ```

---

## üìä Comparativa: Ollama vs APIs Cloud

| Aspecto | Ollama Local | Gemini/OpenAI/NVIDIA |
|---------|--------------|----------------------|
| **Privacidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M√°xima | ‚≠ê‚≠ê‚≠ê Media |
| **Costo** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Gratis | ‚≠ê‚≠ê De pago |
| **Velocidad** | ‚≠ê‚≠ê‚≠ê‚≠ê Depende HW | ‚≠ê‚≠ê‚≠ê‚≠ê Consistente |
| **Calidad (72B)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Comparable GPT-4 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Alta |
| **Offline** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê S√≠ | ‚ùå No |
| **L√≠mites** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Ilimitado | ‚≠ê‚≠ê‚≠ê Cuotas |
| **Facilidad** | ‚≠ê‚≠ê‚≠ê T√©cnico | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Simple |

---

## üéì Recursos Adicionales

- **Documentaci√≥n Oficial**: https://ollama.com/library
- **Modelos Disponibles**: https://ollama.com/library/qwen2.5
- **GitHub de Ollama**: https://github.com/ollama/ollama
- **Comunidad Discord**: https://discord.gg/ollama

---

## üí° Consejos de Uso

### Para M√°ximo Rendimiento

1. **Usa GPU**: Ollama detecta autom√°ticamente tu NVIDIA RTX 5080
2. **Cierra aplicaciones**: Libera RAM/VRAM para el modelo
3. **Deja calentarse el modelo**: Primera consulta puede tardar m√°s

### Para M√°xima Calidad

1. **Usa Qwen2.5 72B**: Mejor para an√°lisis complejos
2. **Configura temperatura**: Baja (0.3-0.5) para respuestas precisas
3. **Proporciona contexto**: M√°s detalles = mejores respuestas

### Para M√°xima Privacidad

1. **Datos locales**: Nada sale de tu m√°quina
2. **Sin telemetr√≠a**: Ollama no env√≠a datos a servidores
3. **GDPR compliant**: Ideal para licitaciones confidenciales

---

## üìû Soporte

Si tienes problemas:

1. **Verifica logs de Ollama**:
   - Windows: `C:\Users\<tu-usuario>\.ollama\logs\server.log`
   - Linux: `~/.ollama/logs/server.log`

2. **Reporta issues**:
   - GitHub TenderAI: https://github.com/tu-repo/issues
   - GitHub Ollama: https://github.com/ollama/ollama/issues

---

## ‚úÖ Checklist Final

Marca cada paso cuando lo completes:

- [ ] Ollama instalado y `ollama --version` funciona
- [ ] Modelo `qwen2.5:72b` descargado (`ollama list`)
- [ ] Embeddings `nomic-embed-text` descargado
- [ ] Servidor Ollama corriendo (`ollama serve`)
- [ ] `langchain-ollama` instalado (`pip list | grep ollama`)
- [ ] Migraciones aplicadas (`python manage.py migrate`)
- [ ] Perfil configurado con provider "Ollama (Local)"
- [ ] Chat funcionando con respuestas de Qwen2.5
- [ ] Indexaci√≥n funcionando con costo ‚Ç¨0.00

---

## üéâ ¬°Listo!

Ahora tienes:
- ‚úÖ Modelo de **calidad GPT-4** corriendo localmente
- ‚úÖ **Privacidad total** - nada sale de tu m√°quina
- ‚úÖ **Costo cero** - sin l√≠mites de uso
- ‚úÖ **Rendimiento excelente** con tu RTX 5080

**¬°Disfruta de TenderAI con Ollama!** ü¶ôüöÄ
